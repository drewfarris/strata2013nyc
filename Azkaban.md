LinkedIN Talk @ Strata NY 2013
==============================

They don't talk about Azkaban very much, but it is a core to their infrastructure. Mentioned in many papers.

Hadoop Workflow Schedules aren't too Sexy.


Software Engineer @ LinkedIN in the Hadoop Engineering Group

Thousands of nodes right now. 

2009 Hadoop Infrastructure Group. 

Hadoop LinkedIN API's / Kafka feeds in / Pig / Voldemort out / Hive Registrations.

50 dog years in hadoop

Mostly the guy to blame for the source code but not the name

How Does LinkedIN use Hadoop
===============

Queryies BI and Data Products.

Features on LinkedIN pages are tied to data products generated by hadoop

Robust A/B testing 

Sample set selection / Test analysis performed in Hadoop orchestrated via 


PYMK (People you may know) - premier data products, core to growth. So good, that people are surprised by the results (accused of e-mail hacking). Constantly improving data flows.

Different Hadoop Jobs that must run reliably multiple times a day in order to present fresh data to users.

Design of Azkaban started in 2009

- Workflows
- Run Jobs
- Job History 
- Web UI

Jobs were run as forked local processes -- simple but easy to use.

### Why not Oozie?

In 2009, Oozie wasn't ready. Evaluated later, but decided not to switch. Oozie had more features, but we could live without those complex features. Usability for Oozie wasn't as good as Azkaban, which was easy to use and understand.

### Who is using Azkaban?

  - First users were Software Engineers (usability not a requirement)
  - Data Scientists (create and munge data for data products)
  - Analysts (business operations / forecasting)
  - Product Managers (creating / touching / scheduling hadoop workflows) 
  
  Smaller companies, everyone had different hats.
  
  
  ### Creating flows:
  
 e.g: peanutbutter.job:
```
  type = pig
  creamy.level=4
  chunky.level1
```  
Core concepts `type` and `dependencies` - will wait for dependencies to run
 
 ```
  A  B
  |  |
  C  |
  \  /
   E
```

Workflows evolve and change constantly. With such rapid development, it became difficult to understant.


- View and track progress of flow when it is running.
- Order of flow execution
- How long it ran relative to entire flow.
- Access job logs from the UI.
- Enable users to understand their workflows and how they operate.
- You can track flows as they run.
- Make watching paint dry a little more fun.

Development in one window, Azkaban in another.

Doesn't only do visualization, also features for customizing execution.
Change settings in the UI.
Immediately availabe for the next execution
Tweak model parameters.

Executing flows is easy, and you can customize many things.
Disable/enable jobs. Good for testing little components. 

- Notification on failure. 
- Notify on successes. 
- What to do when the whole workflow fails.
- Failure Actions: Finish Current / Cancel All / Finish Possible

Parallelism: Skip Executions, Run Concurrently, Pipeline wait for jobs in other flows to complete.
Scheduling / SLA
Abnormal Execution time? kill or notify

#### Failure handling

- Notify 
- Auto Retry (retries, retry.backorr properties)
- Retry Failed Jobs 
- Resume Failed Flows from point of failure.

#### History

- All Flows
- Individual Flow over time.
- How Jobs Have performed over time.
- Better Estimate Capacity; Eke out more performance or detect abnormal behavior

#### Success

- 900 Users
- 10,000 Flows
- 2500 flows executing per day
- 6500 jobs per day. 

Bad news, people stepping on each other's toes. So..

#### Project Isolation

- Delete Project
- Project Logs, who did what, when.
- Permissions (Reading, Writing, Scheduling)

#### Authentication

- LDAP via AD
- XML user manager (e.g TOMCAT)
- Secure / Non-secure

Switch from Hadoop 0.20.2 to 1.0

#### Scaling

_Old Architecture_

- Web UI
- Scheduler Manager
- Flow Manager

_New Architecture_

Web Server

- Web UI
- User Manager / View Manager
- Project Manager / Trigger Manager
- Executor

Talks to:

Executor Server

- Http
- Flow Manager
- Job Type Manager

DB Uses Execution History and State.

#### Dependencies

- Jar Deps Conflicts (Hadoop Pig, Hive Voldemort, KAfka)
- Hard to Maintain
- Hard to grow
- Internal Versions > Open Source

Hard to update the open source version because of internal dependencies. For over a year the project languished.

Decoupled depencencies from Azkaban by using external plugins.
e.g: jobtypes: `pig`, `pig12`, `java`, `hive-core.jar`

JobType Manager passes back classpath and execution directories.

Running this on multiple versions of hadoop using a single instance of Azkaban. Change the plugin.

#### Job Types

- Built In: Command-line, Java, Python
- Pluggable: Pig, Hive
- Non-Hadoop Plugin TYpes: Teradata, MySQL, Voldemort

Replicate oozie's method of running jobs through hadoop tasks if we want to. 

#### Open Source

Redoubled to open source[1]. No internal forked version.

[1]: http://azkaban.github.io/azkaban2/

Linked-in specific code go intp modules.

#### Other features

- Viewer plugins to extends, e.g: HDFS browser. Built in a bunch of decoders.
- Reportal, simple one-stop UI.



#### Upcoming features

- Embedded Flows
- Generic Triggers (Time, Data, etc)
- Generic Actions (Email, http post, etc)
- SoloServer mode (H2? MySQL)
- Reportal Plugin


- Conditional Branching
- Admin Tool
- Pig/Hive Visualizer (Ambrose/Lipstick?)


#### More Information

Azkaban Website[2]

[2]: http://azkaban.github.io/azkaban2/


#### Questions

*Progrmatically Generating Workflow* 

Teams resistant to using web UI or writing job files. Rake tasks to generate the entire workflow for you without using the UI but using the REST UI instead.

*Version Control*

Internally Linkedin uses versioning -- checked in to repository, built and automatically uploaded into azkaban.

*Flows and Jobs*

Discard entire plan instead of simple dependencies. Build pig visaualizer.

Are parameters saved with the execution of the job?
Flows get converted into executable flows.
Time dependencies? How are they expressed? Hourly jobs? Parameter passing - a job can create a parameter that gets passed to subsequent jobs.

Paramter file like when the flow was started, when the job was started.

*Monitoring in Production*

JMX code - slight pain - which is why there is a push for the Admin Server. Right now we need to write `curl` into a web-based JMX interface. Curl to check stats.










